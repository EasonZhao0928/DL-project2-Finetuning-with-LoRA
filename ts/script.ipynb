{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz-1gxT17Bp1"
      },
      "outputs": [],
      "source": [
        "!pip install peft datasets transformers evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfRCm2k97Evk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Optional: for accurate CUDA error traces during debugging\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    RobertaTokenizerFast,\n",
        "    BertForSequenceClassification,\n",
        "    RobertaForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 1) Load AG News and build label mapping\n",
        "raw = load_dataset(\"ag_news\")\n",
        "labels = raw[\"train\"].features[\"label\"].names\n",
        "id2label = {i: lab for i, lab in enumerate(labels)}\n",
        "num_labels = len(labels)\n",
        "\n",
        "# 2) Tokenizers for teacher & student\n",
        "teacher_tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased\")\n",
        "student_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# 3) Preprocess: return both student & teacher encodings + labels\n",
        "def preprocess(examples):\n",
        "    texts = examples[\"text\"]\n",
        "    t_enc = teacher_tokenizer(texts, truncation=True, max_length=512)\n",
        "    s_enc = student_tokenizer(texts, truncation=True, max_length=512)\n",
        "    return {\n",
        "        \"input_ids\":              s_enc[\"input_ids\"],\n",
        "        \"attention_mask\":         s_enc[\"attention_mask\"],\n",
        "        \"teacher_input_ids\":      t_enc[\"input_ids\"],\n",
        "        \"teacher_attention_mask\": t_enc[\"attention_mask\"],\n",
        "        \"labels\":                 examples[\"label\"],\n",
        "    }\n",
        "\n",
        "tokenized_train = raw[\"train\"].map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    load_from_cache_file=False,\n",
        "    remove_columns=raw[\"train\"].column_names\n",
        ")\n",
        "tokenized_eval = raw[\"test\"].map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    load_from_cache_file=False,\n",
        "    remove_columns=raw[\"test\"].column_names\n",
        ")\n",
        "\n",
        "# 4) Collate function: pad both student & teacher inputs\n",
        "def collate_fn(examples):\n",
        "    # Pad student inputs\n",
        "    student_batch = student_tokenizer.pad(\n",
        "        {\n",
        "            \"input_ids\":      [ex[\"input_ids\"] for ex in examples],\n",
        "            \"attention_mask\": [ex[\"attention_mask\"] for ex in examples],\n",
        "        },\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # Pad teacher inputs\n",
        "    teacher_batch = teacher_tokenizer.pad(\n",
        "        {\n",
        "            \"input_ids\":      [ex[\"teacher_input_ids\"] for ex in examples],\n",
        "            \"attention_mask\": [ex[\"teacher_attention_mask\"] for ex in examples],\n",
        "        },\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # Labels tensor\n",
        "    labels = torch.tensor([ex[\"labels\"] for ex in examples], dtype=torch.long)\n",
        "    # Assemble batch\n",
        "    return {\n",
        "        **student_batch,\n",
        "        \"teacher_input_ids\":      teacher_batch[\"input_ids\"],\n",
        "        \"teacher_attention_mask\": teacher_batch[\"attention_mask\"],\n",
        "        \"labels\":                 labels,\n",
        "    }\n",
        "\n",
        "# 5) Set device and load models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Teacher on GPU\n",
        "teacher = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-large-uncased\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        ").to(device)\n",
        "teacher.eval()\n",
        "for p in teacher.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Student base + LoRA, on GPU\n",
        "student_base = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        ")\n",
        "peft_cfg = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\",\n",
        ")\n",
        "student = get_peft_model(student_base, peft_cfg).to(device)\n",
        "\n",
        "# Define a function to print trainable parameters\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    # print(f\"ðŸ“Š Total trainable parameters: {total_trainable_params:,}\")\n",
        "    return total_trainable_params\n",
        "\n",
        "print(f\"teacher trainable parameters: {print_trainable_parameters(teacher)}\")\n",
        "\n",
        "print(f\"student trainable parameters: {print_trainable_parameters(student)}\")\n",
        "\n",
        "# 6) Custom Trainer for distillation\n",
        "class DistillationTrainer(Trainer):\n",
        "    def __init__(self, teacher_model, alpha=0.7, temperature=2.0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher = teacher_model\n",
        "        self.alpha   = alpha\n",
        "        self.temp    = temperature\n",
        "        self.kl_div  = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        # Student forward on GPU\n",
        "        s_out = model(\n",
        "            input_ids=inputs[\"input_ids\"].to(device),\n",
        "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "        )\n",
        "        logits_s = s_out.logits\n",
        "\n",
        "        # Teacher forward on GPU\n",
        "        with torch.no_grad():\n",
        "            t_out = self.teacher(\n",
        "                input_ids=inputs[\"teacher_input_ids\"].to(device),\n",
        "                attention_mask=inputs[\"teacher_attention_mask\"].to(device),\n",
        "            )\n",
        "        logits_t = t_out.logits\n",
        "\n",
        "        # Hard-label cross-entropy\n",
        "        loss_ce = nn.CrossEntropyLoss()(logits_s, labels.to(device))\n",
        "\n",
        "        # Soft-label distillation (KL divergence)\n",
        "        T       = self.temp\n",
        "        log_p_s = nn.functional.log_softmax(logits_s / T, dim=-1)\n",
        "        p_t     = nn.functional.softmax(logits_t / T,      dim=-1)\n",
        "        loss_kd = self.kl_div(log_p_s, p_t) * (T * T)\n",
        "\n",
        "        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kd\n",
        "        return (loss, s_out) if return_outputs else loss\n",
        "\n",
        "# 7) TrainingArguments with unused columns preserved\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"distill_results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=5e-5,\n",
        "    eval_strategy=\"steps\",             # run evaluation every N steps\n",
        "    save_strategy=\"steps\",             # save checkpoint every N steps\n",
        "    logging_strategy=\"steps\",          # log train loss every N steps\n",
        "    logging_steps=100,                 # N = 100 steps\n",
        "    eval_steps=100,                    # eval (and log accuracy) every 100 steps\n",
        "    save_steps=500,                    # save checkpoint every 500 steps\n",
        "    fp16=True,\n",
        "    max_steps=1600,\n",
        "    gradient_accumulation_steps=2,\n",
        "    load_best_model_at_end=True,       # now valid since eval_strategy == save_strategy\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=8,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "# 8) Instantiate & run\n",
        "trainer = DistillationTrainer(\n",
        "    teacher_model=teacher,\n",
        "    alpha=0.7,\n",
        "    temperature=2.0,\n",
        "    model=student,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    data_collator=collate_fn,\n",
        "    tokenizer=student_tokenizer,\n",
        "    compute_metrics=lambda p: {\n",
        "        \"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()\n",
        "    },\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Final evaluation:\", trainer.evaluate())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER3iM2hI8UGN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "from peft import PeftModel\n",
        "\n",
        "# 1) Config\n",
        "BASE_MODEL      = \"roberta-base\"\n",
        "PEFT_CHECKPOINT = \"checkpoint-1600\" # your best LoRA checkpoint\n",
        "TEST_PICKLE     = \"test_unlabelled.pkl\"\n",
        "OUTPUT_CSV      = \"inference_output.csv\"\n",
        "\n",
        "# 2) Tokenizer & collator (student-only)\n",
        "tokenizer     = RobertaTokenizer.from_pretrained(BASE_MODEL)\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "# 3) Load and wrap your student\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "base = RobertaForSequenceClassification.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    num_labels=4\n",
        ")\n",
        "model = PeftModel.from_pretrained(base, PEFT_CHECKPOINT).to(device)\n",
        "model.eval()\n",
        "\n",
        "# 4) Evaluation helper\n",
        "def evaluate_model(inf_model, dataset, batch_size=8, collate_fn=None):\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            # only input_ids & attention_mask will be in batch\n",
        "            inputs = {\n",
        "                \"input_ids\":      batch[\"input_ids\"].to(device),\n",
        "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
        "            }\n",
        "            logits = inf_model(**inputs).logits\n",
        "            preds.append(logits.argmax(dim=-1).cpu())\n",
        "    return torch.cat(preds, dim=0)\n",
        "\n",
        "# 5) Load & tokenize your test set\n",
        "unlabelled = pd.read_pickle(TEST_PICKLE)\n",
        "test_ds = unlabelled.map(\n",
        "    lambda ex: tokenizer(ex[\"text\"], truncation=True, padding=False),\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "# 6) Run inference\n",
        "predictions = evaluate_model(model, test_ds, batch_size=64, collate_fn=data_collator)\n",
        "\n",
        "# 7) Save\n",
        "df = pd.DataFrame({\"ID\": range(len(predictions)), \"Label\": predictions.tolist()})\n",
        "df.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"Inference complete â€” wrote {OUTPUT_CSV}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
